# 位置编码

>不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。

[绝对位置编码和相对位置编码归纳](https://kexue.fm/archives/8130)

> 一般来说，绝对位置编码具有实现简单、计算速度快等优点，而相对位置编码则直接地体现了相对位置信号，跟我们的直观理解吻合，实际性能往往也更好。由此可见，如果可以通过绝对位置编码的方式实现相对位置编码，那么就是“集各家之所长”、“鱼与熊掌兼得”了。Sinusoidal位置编码隐约做到了这一点，但并不够好。旋转式位置编码（Rotary Position Embedding，RoPE）是一种配合Attention机制能达到“绝对位置编码的方式实现相对位置编码”的设计。而也正因为这种设计，它还是目前唯一一种可用于线性 Attention 的相对位置编码。
>
> [线性 Attention](https://kexue.fm/archives/7546)：$softmax(QK^T)V$ => 这个计算复杂度是 $O(n^2)$ 的，一个最简单的线性化方法就是去掉 softmax => $QK^TV$，由于矩阵乘法是满足结合率的，所以我们可以先算 $K^TV$，得到一个 $d*d$ 的矩阵，然后用 $Q$ 左乘它，由于 $d\ll n$，所以这样算的复杂度只是 $O(n)$​
>
> RoPE是目前唯一一种可以用于线性Attention的相对位置编码。这是因为其他的相对位置编码，都是直接基于Attention矩阵进行操作的，但是线性Attention并没有事先算出Attention矩阵，因此也就不存在操作Attention矩阵的做法，所以其他的方案无法应用到线性Attention中。而对于RoPE来说，它是用绝对位置编码的方式来实现相对位置编码，不需要操作Attention矩阵，因此有了应用到线性Attention的可能性。

# 长度外推

[外推介绍](https://kexue.fm/archives/9431)