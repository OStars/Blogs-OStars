# 分词

Tokenization（分词） 在自然语言处理的任务中是最基本的一步，把文本内容处理为最小基本单元即 token，用于后续的处理。分词基本思想是构建一个词表通过词表一一映射进行分词，总的来说分词可以用三个核心函数来描述：

* def train() -> 获得词表
* def encode() -> 根据词表对文本进行分词，并将分词结果映射为词表索引
* def decode() -> 根据词表将一个词表索引序列还原成文本



# 构建词表

## Word level

在英文语系中，word level 分词实现很简单，因为有天然的分隔符。中文里面的 word，需要一些分词工具比如 jieba

```text
中文句子：我喜欢看电影和读书。
分词结果：我 | 喜欢 | 看 | 电影 | 和 | 读书。
英文句子：I enjoy watching movies and reading books.
分词结果：I | enjoy | watching | movies | and | reading | books.
```

word level 的优点有：

* **语义明确**：以词为单位进行分词可以更好地保留每个词的语义，使得文本在后续处理中能够更准确地表达含义。
* **上下文理解**：以词为粒度进行分词有助于保留词语之间的关联性和上下文信息，从而在语义分析和理解时能够更好地捕捉句子的意图。

缺点：

* **长尾效应和稀有词问题**： 词表可能变得巨大，包含很多不常见的词汇，增加存储和训练成本，稀有词的训练数据有限，难以获得准确的表示。
* **OOV（Out-of-Vocabulary）**： 词粒度分词模型只能使用词表中的词来进行处理，无法处理词表之外的词汇，这就是所谓的OOV问题。
* **形态关系和词缀关系**： 无法捕捉同一词的不同形态，也无法有效学习词缀在不同词汇之间的共通性，限制了模型的语言理解能力，比如love和loves在word（词）粒度的词表中将会是两个词。



## Char level

以字符为单位进行分词，即将文本拆分成一个个单独的字符作为最小基本单元，这种字符粒度的分词方法适用于多种语言，无论是英文、中文还是其他不同语言，都能够一致地使用字符粒度进行处理，因为英文就26个字母以及其他的一些符号，中文常见字就 6000 个左右。

```text
中文句子：我喜欢看电影和读书。
分词结果：我 | 喜 | 欢 | 看 | 电 | 影 | 和 | 读 | 书 | 。

英文句子：I enjoy watching movies and reading books.
分词结果：I |   | e | n | j | o | y |   | w | a | t | c | h | i | n | g |   | m | o | v | i | e | s |   | a | n | d |   | r | e | a | d | i | n | g |   | b | o | o | k | s | .
```

char level 的优点有：

* **统一处理方式**：字符粒度分词方法适用于不同语言，无需针对每种语言设计不同的分词规则或工具，具有通用性。
* **解决OOV问题**：由于字符粒度分词可以处理任何字符，无需维护词表，因此可以很好地处理一些新创词汇、专有名词等问题。

缺点：

* **语义信息不明确**：字符粒度分词无法直接表达词的语义，可能导致在一些语义分析任务中效果较差。
* **处理效率低**：由于文本被拆分为字符，处理的粒度较小，增加后续处理的计算成本和时间。



## Subword level

在很多情况下，既不希望将文本切分成单独的词（太大），也不想将其切分成单个字符（太小），而是希望得到介于词和字符之间的子词单元。这就引入了 subword level 的分词方法。像 wordpieces、BPE、BBPE 都属于 subword level 的分词方法，实现方法略有不同

### wordpieces

WordPiece核心思想是将单词拆分成多个前缀符号（比如BERT中的##）最小单元，再通过子词合并规则将最小单元进行合并为子词级别。例如对于单词"word"，拆分如下：

```text
w ##o ##r ##d
```

然后通过合并规则进行合并，从而循环迭代构建出一个词表，以下是核心步骤：

1. 计算初始词表：通过训练语料获得或者最初的英文中 26 个字母加上各种符号以及常见中文字符，这些作为初始词表。

2. 计算合并分数：对训练语料拆分的多个子词单元通过合拼规则计算合并分数。

   ```python
   score = (freq_of_pair) / (freq_of_first_element × freq_of_second_element)
   ```

3. 合并分数最高的子词对：选择分数最高的子词对，将它们合并成一个新的子词单元，并更新词表。

4. 重复合并步骤：不断重复步骤 2 和步骤 3，直到达到预定的词表大小、合并次数，或者直到不再有有意义的合并（即，进一步合并不会显著提高词表的效益）。

5. 分词：使用最终得到的词汇表对文本进行分词。

### BPE

Byte-Pair Encoding (BPE) 核心思想是**逐步合并出现频率最高的子词对**，从而构造出一个词表

假设我们手头有一堆文档 $D={d_1,d_2,…}$

1. 把每个文档 d 变成单词列表，比如你可以简单用空格分词
2. 统计每个单词 w 在*所有*文档 D 中的出现频率，并计算初始字符集 `alphabet` 作为一开始的 Vocab（包括后面的 `</w>`），字符集的意思就是*所有*文档 D 中*不同*的字符集合
3. 先将每个单词划分为一个个 utf-8 char，称为一个划分，比如 `highest -> h, i, g, h, e, s, t`
4. 然后，在每个单词的划分最后面加上 `</w>`，那么现在 `highest -> h, i, g, h, e, s, t, </w>`
5. 重复下面步骤直到满足两个条件中的任意一个：1）Vocab 达到上限。2）达到最大迭代次数
   1. 找到**最经常一起出现的 pair**，并记录这个合并规则，放在 merge table 里面，同时把合并之后的结果放到 Vocab 里面
   2. 更新所有单词的划分，假设我们发现 `(h, i)` 最经常一起出现，那么 `hi` 就会被添加到 Vocab 里面，同时修改划分方式为：`highest -> hi, g, h, e, s, t, </w>`

**从算法流程可以看到，BPE 分词算法合并最经常一起出现的 pair 的时候，并不会跨越单词**

在 BPE 完成训练之后，我们会得到一个 merge table，也会得到一个 Vocab，假设现在要处理文本 `s`

1. 和训练的时候采用一样的方法，先把 `s` 拆分成单词列表，每个单词拆分为一个个 utf-8 char，记得每个单词的划分最后要加上 `</w>`
2. 遍历 merge table，并检查每个合并规则是否可以用来更新每个单词的划分，可以的话就合并更新

这里有个细节：之前我们是找*最经常一起出现的 pair* 并记录这个合并规则。所以*按序遍历* merge table 就已经*隐含*了「优先合并最经常出现的 pair」这件事了

### BBPE

Byte-level BPE (BBPE) 和 Byte-Pair Encoding (BPE) 区别就是 BPE 是最小词汇是字符级别，而 BBPE 是字节级别的，通过 UTF-8 的方式编码，理论上可以表示这个世界上的所有字符。所以实现的步骤和BPE就是实现的粒度不一样，其他的都是一样的。

